"""
explore.py - Exploration, d√©tection d'√©mergence et d'anomalie FPS
Version exhaustive conforme √† la feuille de route FPS V1.3
---------------------------------------------------------------
NOTE FPS ‚Äì Plasticit√© m√©thodologique :
Ce module capture les ph√©nom√®nes non anticip√©s ou √©mergents.
Il doit rester ouvert, extensible, et permettre √† chaque contributeur
d'ajouter ses propres d√©tecteurs ou analyses.
---------------------------------------------------------------

Ce module r√©v√®le l'invisible dans la dynamique FPS :
- Anomalies persistantes et √©v√©nements chaotiques
- Bifurcations spiral√©es et transitions de phase
- √âmergences harmoniques et nouveaux motifs
- Patterns fractals et auto-similarit√©
- Cycles attracteurs dans l'espace de phase

Toute √©mergence d√©tect√©e est logu√©e, tra√ßable (avec seed/config),
et sujette √† reproductibilit√©.

(c) 2025 Gepetto & Andr√©a Gadal & Claude üåÄ
"""

import numpy as np
import pandas as pd
import json
import csv
import os
from datetime import datetime
from typing import Dict, List, Tuple, Optional, Any, Union
from scipy import signal, spatial
from scipy.stats import entropy
import warnings
from collections import defaultdict
from utils import deep_convert

# Imports pour coh√©rence avec les autres modules
try:
    import h5py
    HDF5_AVAILABLE = True
except ImportError:
    HDF5_AVAILABLE = False
    warnings.warn("h5py non disponible - lecture HDF5 d√©sactiv√©e")


# ============== ORCHESTRATION PRINCIPALE ==============

def run_exploration(run_data_path: str, output_dir: str, 
                   config: Optional[Dict] = None) -> Dict[str, Any]:
    """
    Orchestration compl√®te de l'exploration post-run.
    
    üîß CORRECTION : Ajout diversification bas√©e sur seed pour √©viter explorations identiques
    
    1. Charge les logs du run (CSV/HDF5)
    2. Lance tous les d√©tecteurs sur les m√©triques d√©finies
    3. Agr√®ge les √©v√©nements et les exporte
    4. G√©n√®re un rapport Markdown d√©taill√©
    
    Args:
        run_data_path: chemin vers les logs CSV ou HDF5
        output_dir: dossier de sortie pour les r√©sultats
        config: configuration (optionnelle, sinon charg√©e depuis config.json)
    
    Returns:
        Dict avec tous les r√©sultats d'exploration
    """
    print(f"\n=== Exploration FPS : {os.path.basename(run_data_path)} ===")
    
    # Cr√©er le dossier de sortie
    os.makedirs(output_dir, exist_ok=True)
    
    # Charger la configuration si non fournie
    if config is None:
        config = load_config_for_exploration()
    
    exploration_config = config.get('exploration', {})
    
    # üîß CORRECTION : Extraire seed du filename pour diversifier l'exploration
    run_id = extract_run_id(run_data_path)
    seed_from_filename = extract_seed_from_filename(run_data_path)
    
    # Diversifier les param√®tres d'exploration bas√©s sur la seed
    if seed_from_filename:
        print(f"üå± Diversification exploration bas√©e sur seed: {seed_from_filename}")
        np.random.seed(seed_from_filename)
        
        # Modifier l√©g√®rement les seuils pour chaque seed
        seed_factor = (seed_from_filename % 1000) / 1000.0  # 0.0 - 0.999
        anomaly_threshold = exploration_config.get('anomaly_threshold', 3.0) * (0.8 + 0.4 * seed_factor)
        fractal_threshold = exploration_config.get('fractal_threshold', 0.8) * (0.7 + 0.3 * seed_factor)
        
        print(f"   Seuils diversifi√©s - anomaly: {anomaly_threshold:.2f}, fractal: {fractal_threshold:.2f}")
    else:
        anomaly_threshold = exploration_config.get('anomaly_threshold', 3.0)
        fractal_threshold = exploration_config.get('fractal_threshold', 0.8)
    
    # Charger les donn√©es du run
    print("üìä Chargement des donn√©es...")
    data = load_run_data(run_data_path)
    
    if not data:
        print("‚ùå Impossible de charger les donn√©es")
        return deep_convert({'status': 'error', 'message': 'Donn√©es non charg√©es'})
    
    # üîß CORRECTION : Ajouter une petite randomisation aux donn√©es pour diff√©rencier les analyses
    # Cela ne change pas les donn√©es fondamentales mais permet d'√©viter les patterns exactement identiques
    if seed_from_filename:
        data = add_exploration_diversity(data, seed_from_filename)
    
    # Collecter tous les √©v√©nements
    all_events = []
    
    # 1. D√©tection d'anomalies (avec seuil diversifi√©)
    if exploration_config.get('detect_anomalies', True):
        print("\nüîç D√©tection d'anomalies...")
        anomalies = detect_anomalies(
            data,
            exploration_config.get('metrics', ['S(t)', 'C(t)', 'effort(t)']),
            anomaly_threshold,  # Utiliser seuil diversifi√©
            exploration_config.get('min_duration', 3)
        )
        all_events.extend(anomalies)
        print(f"  ‚Üí {len(anomalies)} anomalies d√©tect√©es")
    
    # 2. D√©tection de bifurcations spiral√©es (avec param√®tres diversifi√©s)
    print("\nüåÄ D√©tection de bifurcations...")
    phase_threshold = np.pi * (0.8 + 0.4 * (seed_factor if seed_from_filename else 0.5))
    bifurcations = detect_spiral_bifurcations(
        data,
        phase_metric='C(t)',
        threshold=phase_threshold
    )
    all_events.extend(bifurcations)
    print(f"  ‚Üí {len(bifurcations)} bifurcations d√©tect√©es")
    
    # 3. D√©tection d'√©mergences harmoniques (avec fen√™tres diversifi√©es)
    if exploration_config.get('detect_harmonics', True):
        print("\nüéµ D√©tection d'√©mergences harmoniques...")
        window_size = int(100 * (0.8 + 0.4 * (seed_factor if seed_from_filename else 0.5)))
        harmonics = detect_harmonic_emergence(
            data,
            signal_metric='S(t)',
            n_harmonics=5,
            window=window_size,
            step=10
        )
        all_events.extend(harmonics)
        print(f"  ‚Üí {len(harmonics)} √©mergences harmoniques")
    
    # 4. Exploration de l'espace de phase (diversifi√©e)
    print("\nüìà Exploration de l'espace de phase...")
    phase_window = int(50 * (0.7 + 0.6 * (seed_factor if seed_from_filename else 0.5)))
    phase_events = explore_phase_space(
        data,
        metric='S(t)',
        window=phase_window,
        min_diagonal_length=5
    )
    all_events.extend(phase_events)
    print(f"  ‚Üí {len(phase_events)} patterns dans l'espace de phase")
    
    # 5. D√©tection de motifs fractals (avec seuil diversifi√©)
    fractal_events = []  # Initialiser la variable
    if exploration_config.get('detect_fractal_patterns', True):
        print("\nüåø D√©tection de motifs fractals...")
        fractal_events = detect_fractal_patterns(
            data,
            metrics=exploration_config.get('metrics', ['S(t)', 'C(t)', 'effort(t)']),
            window_sizes=exploration_config.get('window_sizes', [1, 10, 100]),
            threshold=fractal_threshold  # Utiliser seuil diversifi√©
        )
        all_events.extend(fractal_events)
        print(f"  ‚Üí {len(fractal_events)} motifs fractals d√©tect√©s")
        
        # Logger les √©v√©nements fractals s√©par√©ment
        if fractal_events:
            fractal_log_path = os.path.join(output_dir, f"fractal_events_{run_id}.csv")
            log_fractal_events(fractal_events, fractal_log_path)
    
    # 6. Exporter tous les √©v√©nements
    events_path = os.path.join(output_dir, f"emergence_events_{run_id}.csv")
    log_events(all_events, events_path)
    print(f"\nüíæ √âv√©nements export√©s : {events_path}")
    
    # 7. G√©n√©rer le rapport
    report_path = os.path.join(output_dir, f"exploration_report_{run_id}.md")
    generate_report(all_events, report_path, run_id, config)
    print(f"üìÑ Rapport g√©n√©r√© : {report_path}")
    
    # R√©sum√© des r√©sultats
    results = {
        'status': 'success',
        'run_id': run_id,
        'seed_used': seed_from_filename,
        'total_events': len(all_events),
        'events_by_type': count_events_by_type(all_events),
        'events': all_events,
        'diversified_params': {
            'anomaly_threshold': anomaly_threshold,
            'fractal_threshold': fractal_threshold,
            'phase_threshold': phase_threshold if seed_from_filename else np.pi
        },
        'paths': {
            'events': events_path,
            'report': report_path,
            'fractal_events': os.path.join(output_dir, f"fractal_events_{run_id}.csv") if fractal_events else None
        }
    }
    
    print(f"\n‚úÖ Exploration termin√©e : {len(all_events)} √©v√©nements d√©tect√©s")
    
    return deep_convert(results)


# ============== CHARGEMENT DES DONN√âES ==============

def load_run_data(data_path: str) -> Dict[str, np.ndarray]:
    """
    Charge les donn√©es depuis CSV ou HDF5.
    """
    if data_path.endswith('.csv'):
        return deep_convert(load_csv_data(data_path))
    elif data_path.endswith('.h5') or data_path.endswith('.hdf5'):
        if HDF5_AVAILABLE:
            return deep_convert(load_hdf5_data(data_path))
        else:
            warnings.warn("HDF5 non disponible - impossible de lire le fichier")
            return {}
    else:
        warnings.warn(f"Format non reconnu : {data_path}")
        return {}


def load_csv_data(csv_path: str) -> Dict[str, np.ndarray]:
    """
    Charge les donn√©es depuis un fichier CSV.
    """
    data = defaultdict(list)
    
    try:
        with open(csv_path, 'r') as f:
            reader = csv.DictReader(f)
            for row in reader:
                for key, value in row.items():
                    try:
                        # Convertir en float si possible
                        if value and value.lower() not in ['stable', 'transitoire', 'chronique']:
                            data[key].append(float(value))
                        else:
                            data[key].append(value)
                    except ValueError:
                        data[key].append(value)
        
        # Convertir en arrays numpy
        for key in data:
            if data[key] and isinstance(data[key][0], (int, float)):
                data[key] = np.array(data[key])
        
        return deep_convert(dict(data))
        
    except Exception as e:
        warnings.warn(f"Erreur chargement CSV : {e}")
        return {}


def load_hdf5_data(hdf5_path: str) -> Dict[str, np.ndarray]:
    """
    Charge les donn√©es depuis un fichier HDF5.
    """
    data = {}
    
    try:
        with h5py.File(hdf5_path, 'r') as f:
            # Parcourir tous les groupes temporels
            for time_key in f.keys():
                group = f[time_key]
                
                # Extraire les m√©triques
                for metric in group.attrs:
                    if metric not in data:
                        data[metric] = []
                    data[metric].append(group.attrs[metric])
                
                # Extraire les datasets
                for dataset_name in group.keys():
                    if dataset_name not in data:
                        data[dataset_name] = []
                    data[dataset_name].append(group[dataset_name][:])
        
        # Convertir en arrays
        for key in data:
            data[key] = np.array(data[key])
        
        return deep_convert(data)
        
    except Exception as e:
        warnings.warn(f"Erreur chargement HDF5 : {e}")
        return {}


# ============== D√âTECTION D'ANOMALIES ==============

def detect_anomalies(data: Dict[str, np.ndarray], metrics: List[str], 
                     threshold: float = 3.0, min_duration: int = 3) -> List[Dict]:
    """
    D√©tecte les s√©quences persistantes de valeurs hors-norme.
    
    Une anomalie est une d√©viation > threshold * œÉ pendant min_duration pas.
    
    Args:
        data: donn√©es du run
        metrics: m√©triques √† analyser
        threshold: seuil en nombre d'√©carts-types
        min_duration: dur√©e minimale de persistance
    
    Returns:
        Liste d'√©v√©nements anomalies
    """
    events = []
    
    for metric in metrics:
        if metric not in data:
            continue
        
        values = data[metric]
        if len(values) < 20:  # Pas assez de donn√©es
            continue
        
        # Statistiques glissantes
        window_size = min(50, len(values) // 4)
        
        for i in range(window_size, len(values)):
            # Fen√™tre de r√©f√©rence
            window = values[i-window_size:i]
            mean_w = np.mean(window)
            std_w = np.std(window)
            
            if std_w < 1e-10:  # √âviter division par z√©ro
                continue
            
            # D√©tecter le d√©but d'une anomalie
            z_score = abs(values[i] - mean_w) / std_w
            
            if z_score > threshold:
                # Chercher la dur√©e de l'anomalie
                duration = 1
                max_z = z_score
                
                for j in range(i+1, min(i+50, len(values))):
                    z_j = abs(values[j] - mean_w) / std_w
                    if z_j > threshold:
                        duration += 1
                        max_z = max(max_z, z_j)
                    else:
                        break
                
                # Enregistrer si dur√©e suffisante
                if duration >= min_duration:
                    events.append({
                        'event_type': 'anomaly',
                        't_start': i,
                        't_end': i + duration - 1,
                        'metric': metric,
                        'value': float(max_z),
                        'severity': classify_severity(max_z, threshold)
                    })
                    
                    # Sauter √† la fin de l'anomalie
                    i += duration
    
    return deep_convert(events)


# ============== D√âTECTION DE BIFURCATIONS ==============

def detect_spiral_bifurcations(data: Dict[str, np.ndarray], 
                               phase_metric: str = 'C(t)',
                               threshold: float = np.pi) -> List[Dict]:
    """
    Analyse les changements de phase/bifurcations dans la m√©trique d'accord spiral√©.
    
    Une bifurcation est un changement brusque de la dynamique de phase.
    
    Args:
        data: donn√©es du run
        phase_metric: m√©trique de phase √† analyser
        threshold: seuil de changement de phase
    
    Returns:
        Liste d'√©v√©nements bifurcation
    """
    events = []
    
    if phase_metric not in data:
        return deep_convert(events)
    
    values = data[phase_metric]
    if len(values) < 10:
        return deep_convert(events)
    
    # Calculer la d√©riv√©e de la phase
    phase_derivative = np.gradient(values)
    
    # D√©tecter les changements brusques
    for i in range(1, len(phase_derivative)-1):
        # Changement de signe de la d√©riv√©e
        if phase_derivative[i-1] * phase_derivative[i+1] < 0:
            # Amplitude du changement
            change = abs(values[i+1] - values[i-1])
            
            if change > threshold / 10:  # Seuil adaptatif
                events.append({
                    'event_type': 'bifurcation',
                    't_start': i-1,
                    't_end': i+1,
                    'metric': phase_metric,
                    'value': float(change),
                    'severity': 'medium' if change < threshold else 'high'
                })
    
    # D√©tecter aussi les sauts de phase
    phase_diff = np.diff(values)
    for i, diff in enumerate(phase_diff):
        if abs(diff) > threshold:
            events.append({
                'event_type': 'phase_jump',
                't_start': i,
                't_end': i+1,
                'metric': phase_metric,
                'value': float(abs(diff)),
                'severity': 'high'
            })
    
    return deep_convert(events)


# ============== D√âTECTION D'√âMERGENCES HARMONIQUES ==============

def detect_harmonic_emergence(data: Dict[str, np.ndarray], 
                              signal_metric: str = 'S(t)',
                              n_harmonics: int = 5,
                              window: int = 100,
                              step: int = 10) -> List[Dict]:
    """
    Utilise une FFT glissante pour d√©tecter l'apparition de nouvelles harmoniques.
    
    Args:
        data: donn√©es du run
        signal_metric: signal √† analyser
        n_harmonics: nombre d'harmoniques principales √† suivre
        window: taille de la fen√™tre FFT
        step: pas de glissement
    
    Returns:
        Liste d'√©v√©nements harmoniques
    """
    events = []
    
    if signal_metric not in data:
        return deep_convert(events)
    
    values = data[signal_metric]
    if len(values) < window:
        return deep_convert(events)
    
    # FFT glissante
    prev_harmonics = None
    
    for i in range(0, len(values) - window, step):
        # Fen√™tre actuelle
        segment = values[i:i+window]
        
        # FFT
        fft_vals = np.fft.fft(segment)
        fft_abs = np.abs(fft_vals[:window//2])
        
        # Trouver les pics principaux
        peaks, properties = signal.find_peaks(fft_abs, height=np.max(fft_abs)*0.1)
        
        if len(peaks) > 0:
            # Garder les n_harmonics plus fortes
            sorted_peaks = peaks[np.argsort(properties['peak_heights'])[-n_harmonics:]]
            current_harmonics = set(sorted_peaks)
            
            if prev_harmonics is not None:
                # Nouvelles harmoniques apparues
                new_harmonics = current_harmonics - prev_harmonics
                
                if new_harmonics:
                    events.append({
                        'event_type': 'harmonic_emergence',
                        't_start': i,
                        't_end': i + window,
                        'metric': signal_metric,
                        'value': len(new_harmonics),
                        'severity': 'low' if len(new_harmonics) == 1 else 'medium'
                    })
            
            prev_harmonics = current_harmonics
    
    return deep_convert(events)


# ============== EXPLORATION DE L'ESPACE DE PHASE ==============

def explore_phase_space(data: Dict[str, np.ndarray], 
                        metric: str = 'S(t)',
                        window: int = 50,
                        min_diagonal_length: int = 5) -> List[Dict]:
    """
    Recurrence plot : cherche les motifs r√©currents/cycles attracteurs.
    
    Args:
        data: donn√©es du run
        metric: m√©trique √† analyser
        window: taille de l'embedding
        min_diagonal_length: longueur minimale des diagonales
    
    Returns:
        Liste d'√©v√©nements de l'espace de phase
    """
    events = []
    
    if metric not in data:
        return deep_convert(events)
    
    values = data[metric]
    if len(values) < window * 2:
        return deep_convert(events)
    
    # Cr√©er la matrice de r√©currence
    embedding_dim = 3
    delay = 5
    
    # Embedding de Takens
    embedded = []
    for i in range(len(values) - (embedding_dim-1)*delay):
        point = [values[i + j*delay] for j in range(embedding_dim)]
        embedded.append(point)
    
    if len(embedded) < 10:
        return deep_convert(events)
    
    embedded = np.array(embedded)
    
    # Matrice de distances
    distances = spatial.distance_matrix(embedded, embedded)
    
    # Seuil de r√©currence (10% des plus petites distances)
    threshold = np.percentile(distances.flatten(), 10)
    recurrence_matrix = distances < threshold
    
    # Chercher les structures diagonales (cycles)
    for i in range(len(recurrence_matrix) - min_diagonal_length):
        # Diagonale principale
        diagonal_length = 0
        for j in range(min(len(recurrence_matrix) - i, 50)):
            if recurrence_matrix[i+j, j]:
                diagonal_length += 1
            else:
                if diagonal_length >= min_diagonal_length:
                    events.append({
                        'event_type': 'phase_cycle',
                        't_start': i,
                        't_end': i + diagonal_length,
                        'metric': metric,
                        'value': diagonal_length,
                        'severity': 'low' if diagonal_length < 10 else 'medium'
                    })
                diagonal_length = 0
    
    return deep_convert(events)


# ============== D√âTECTION DE MOTIFS FRACTALS ==============

def detect_fractal_patterns(data: Dict[str, np.ndarray],
                            metrics: List[str] = ['S(t)', 'C(t)', 'effort(t)'],
                            window_sizes: List[int] = [1, 10, 100],
                            threshold: float = 0.8) -> List[Dict]:
    """
    Analyse multi-√©chelles pour d√©tecter l'auto-similarit√©.
    
    D√©tecte les p√©riodes o√π le signal pr√©sente des motifs similaires
    √† diff√©rentes √©chelles temporelles.
    
    Args:
        data: donn√©es du run
        metrics: m√©triques √† analyser
        window_sizes: √©chelles √† comparer
        threshold: seuil de similarit√©
    
    Returns:
        Liste d'√©v√©nements fractals
    """
    events = []
    
    for metric in metrics:
        if metric not in data:
            continue
        
        values = data[metric]
        if len(values) < max(window_sizes) * 2:
            continue
        
        # Analyser chaque paire d'√©chelles
        for i in range(len(window_sizes)-1):
            small_window = window_sizes[i]
            large_window = window_sizes[i+1]
            
            # Parcourir le signal
            for t in range(large_window, len(values) - large_window, large_window//2):
                # Extraire les motifs √† diff√©rentes √©chelles
                small_pattern = values[t:t+small_window]
                large_pattern = values[t:t+large_window]
                
                # Sous-√©chantillonner le grand motif
                downsampled = signal.resample(large_pattern, len(small_pattern))
                
                # Calculer la corr√©lation
                if np.std(small_pattern) > 1e-10 and np.std(downsampled) > 1e-10:
                    correlation = np.corrcoef(small_pattern, downsampled)[0, 1]
                    
                    if abs(correlation) > threshold:
                        events.append({
                            'event_type': 'fractal_pattern',
                            't_start': t,
                            't_end': t + large_window,
                            'metric': metric,
                            'value': float(abs(correlation)),
                            'severity': 'medium' if abs(correlation) < 0.9 else 'high',
                            'scale': f"{small_window}/{large_window}"
                        })
        
        # Dimension fractale par box-counting
        if len(values) >= 1000:
            frac_dim = estimate_fractal_dimension(values)
            if 1.2 < frac_dim < 1.8:  # Dimension fractale non-triviale
                events.append({
                    'event_type': 'fractal_dimension',
                    't_start': 0,
                    't_end': len(values)-1,
                    'metric': metric,
                    'value': float(frac_dim),
                    'severity': 'high'
                })
    
    return deep_convert(events)


def estimate_fractal_dimension(data: np.ndarray, max_box_size: int = 100) -> float:
    """
    Estime la dimension fractale par la m√©thode box-counting.
    """
    # Normaliser les donn√©es
    data_norm = (data - np.min(data)) / (np.max(data) - np.min(data) + 1e-10)
    
    box_sizes = []
    counts = []
    
    for box_size in range(2, min(max_box_size, len(data)//10), 2):
        # Compter les bo√Ætes occup√©es
        n_boxes = len(data) // box_size
        occupied = 0
        
        for i in range(n_boxes):
            box_data = data_norm[i*box_size:(i+1)*box_size]
            if np.ptp(box_data) > 0:
                occupied += 1
        
        if occupied > 0:
            box_sizes.append(box_size)
            counts.append(occupied)
    
    if len(box_sizes) > 3:
        # R√©gression log-log
        log_sizes = np.log(box_sizes)
        log_counts = np.log(counts)
        
        # Pente = -dimension fractale
        slope, _ = np.polyfit(log_sizes, log_counts, 1)
        return -slope
    
    return 1.0  # Dimension par d√©faut


# ============== FORMATAGE ET EXPORT ==============

def format_value_for_csv(value: Any) -> str:
    """
    Assure l'export correct de valeurs complexes dans les logs.
    """
    if isinstance(value, (list, np.ndarray)):
        return json.dumps(deep_convert(value.tolist() if isinstance(value, np.ndarray) else value))
    elif isinstance(value, dict):
        return json.dumps(deep_convert(value))
    elif isinstance(value, float):
        return f"{value:.6f}"
    else:
        return str (value)


def log_events(events: List[Dict], csv_path: str) -> None:
    """
    √âcrit le log CSV des √©mergences.
    
    Colonnes : event_type, t_start, t_end, metric, value, severity
    """
    if not events:
        return
    
    os.makedirs(os.path.dirname(csv_path) if os.path.dirname(csv_path) else ".", 
                exist_ok=True)
    
    with open(csv_path, 'w', newline='') as f:
        fieldnames = ['event_type', 't_start', 't_end', 'metric', 'value', 'severity']
        
        # Ajouter les champs suppl√©mentaires si pr√©sents
        extra_fields = set()
        for event in events:
            extra_fields.update(set(event.keys()) - set(fieldnames))
        fieldnames.extend(sorted(extra_fields))
        
        writer = csv.DictWriter(f, fieldnames=fieldnames)
        writer.writeheader()
        
        for event in events:
            row = {}
            for field in fieldnames:
                if field in event:
                    row[field] = format_value_for_csv(event[field])
                else:
                    row[field] = ''
            writer.writerow(row)


def log_fractal_events(events: List[Dict], csv_path: str) -> None:
    """
    Log sp√©cifique pour les √©v√©nements fractals.
    """
    fractal_events = [e for e in events if 'fractal' in e.get('event_type', '')]
    if fractal_events:
        log_events(fractal_events, csv_path)


# ============== G√âN√âRATION DU RAPPORT ==============

def generate_report(events: List[Dict], report_path: str, 
                    run_id: str, config: Dict) -> None:
    """
    G√©n√®re un rapport Markdown d√©taill√©.
    """
    os.makedirs(os.path.dirname(report_path) if os.path.dirname(report_path) else ".", 
                exist_ok=True)
    
    with open(report_path, 'w') as f:
        # En-t√™te
        f.write(f"# Rapport d'exploration FPS\n\n")
        f.write(f"**Run ID :** {run_id}\n")
        f.write(f"**Date :** {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\n")
        f.write(f"**Total √©v√©nements :** {len(events)}\n\n")
        
        # R√©sum√© par type
        f.write("## R√©sum√© par type d'√©v√©nement\n\n")
        event_counts = count_events_by_type(events)
        
        for event_type, count in event_counts.items():
            f.write(f"- **{event_type}** : {count} √©v√©nements\n")
        f.write("\n")
        
        # D√©tails par type
        for event_type in event_counts:
            type_events = [e for e in events if e['event_type'] == event_type]
            
            f.write(f"## {event_type.replace('_', ' ').title()}\n\n")
            
            # Top 5 par s√©v√©rit√©/valeur
            top_events = sorted(type_events, key=lambda x: x.get('value', 0), reverse=True)[:5]
            
            for i, event in enumerate(top_events, 1):
                f.write(f"### {i}. t={event['t_start']}-{event['t_end']}\n")
                f.write(f"- **M√©trique :** {event['metric']}\n")
                f.write(f"- **Valeur :** {event['value']:.4f}\n")
                f.write(f"- **S√©v√©rit√© :** {event['severity']}\n")
                
                # Champs suppl√©mentaires
                for key, value in event.items():
                    if key not in ['event_type', 't_start', 't_end', 'metric', 'value', 'severity']:
                        f.write(f"- **{key} :** {value}\n")
                f.write("\n")
        
        # Section sp√©ciale pour les motifs fractals
        fractal_events = [e for e in events if 'fractal' in e.get('event_type', '')]
        if fractal_events:
            f.write("## Motifs fractals d√©tect√©s\n\n")
            f.write(f"**Nombre total :** {len(fractal_events)}\n\n")
            
            # Grouper par m√©trique
            by_metric = defaultdict(list)
            for event in fractal_events:
                by_metric[event['metric']].append(event)
            
            for metric, metric_events in by_metric.items():
                f.write(f"### {metric}\n")
                f.write(f"- Patterns d√©tect√©s : {len(metric_events)}\n")
                
                # Statistiques de corr√©lation
                correlations = [e['value'] for e in metric_events if 'pattern' in e['event_type']]
                if correlations:
                    f.write(f"- Corr√©lation moyenne : {np.mean(correlations):.3f}\n")
                    f.write(f"- Corr√©lation max : {np.max(correlations):.3f}\n")
                
                # Dimension fractale si pr√©sente
                dim_events = [e for e in metric_events if e['event_type'] == 'fractal_dimension']
                if dim_events:
                    f.write(f"- Dimension fractale : {dim_events[0]['value']:.3f}\n")
                f.write("\n")
        
        # Configuration utilis√©e
        f.write("## Configuration d'exploration\n\n")
        f.write("```json\n")
        f.write(json.dumps(deep_convert(config.get('exploration', {})), indent=2))
        f.write("\n```\n")



# ============== CORR√âLATIONS ==============

def export_all_correlations(history: List[Dict],
                            output_csv: str = None,
                            output_json: str = None,
                            metrics_to_analyze: List[str] = None):
    """
    Exporte TOUTES les corr√©lations entre m√©triques en CSV et/ou JSON.
    
    Args:
        history: historique complet
        output_csv: chemin pour le CSV (None = pas de CSV)
        output_json: chemin pour le JSON (None = pas de JSON)
        metrics_to_analyze: liste des m√©triques (None = toutes)
        
    Returns:
        DataFrame: table de toutes les corr√©lations
    """
    if not history or len(history) < 20:
        print("‚ö†Ô∏è Pas assez d'historique")
        return None
    
    print("üíæ Export de toutes les corr√©lations...")
    
    # M√©triques par d√©faut
    if metrics_to_analyze is None:
        metrics_to_analyze = [
            'S(t)', 'C(t)', 'E(t)',
            'effort(t)', 'entropy_S', 'fluidity',
            'mean_abs_error', 'variance_d2S', 'std_S',
            'gamma', 'gamma_mean(t)',
            'An_mean(t)', 'fn_mean(t)',
            'En_mean(t)', 'On_mean(t)', 'In_mean(t)',
            'tau_A_mean', 'tau_f_mean', 'tau_S', 'tau_gamma', 'tau_C',
            'temporal_coherence', 'adaptive_resilience', 'continuous_resilience',
            'best_pair_score', 'best_pair_gamma',
            'decorrelation_time', 'autocorr_tau',
            'mean_high_effort', 'd_effort_dt', 'max_median_ratio'
        ]
    
    # Cr√©er DataFrame
    data = {}
    for metric in metrics_to_analyze:
        values = []
        for h in history:
            val = h.get(metric)
            values.append(float(val) if val is not None else np.nan)
        data[metric] = values
    
    df = pd.DataFrame(data)
    
    # Supprimer colonnes avec trop de NaN
    df_clean = df.dropna(axis=1, thresh=len(df) * 0.5)
    
    print(f"‚úì {df_clean.shape[1]} m√©triques valides")
    
    # Calculer matrice de corr√©lation
    corr_matrix = df_clean.corr()
    
    # Cr√©er table de toutes les paires
    all_correlations = []
    
    for i in range(len(corr_matrix)):
        for j in range(i+1, len(corr_matrix)):  # i+1 pour √©viter les doublons
            metric1 = corr_matrix.columns[i]
            metric2 = corr_matrix.columns[j]
            corr_val = corr_matrix.iloc[i, j]
            
            if not np.isnan(corr_val):
                all_correlations.append({
                    'metric_1': metric1,
                    'metric_2': metric2,
                    'correlation': corr_val,
                    'abs_correlation': abs(corr_val),
                    'correlation_type': 'positive' if corr_val > 0 else 'negative',
                    'strength': (
                        'very_strong' if abs(corr_val) > 0.9 else
                        'strong' if abs(corr_val) > 0.7 else
                        'moderate' if abs(corr_val) > 0.5 else
                        'weak' if abs(corr_val) > 0.3 else
                        'very_weak'
                    )
                })
    
    # Cr√©er DataFrame
    df_corr = pd.DataFrame(all_correlations)
    
    # Trier par valeur absolue (plus forte en premier)
    df_corr = df_corr.sort_values('abs_correlation', ascending=False)
    
    # Export CSV
    if output_csv:
        df_corr.to_csv(output_csv, index=False)
        print(f"‚úÖ CSV sauvegard√©: {output_csv}")
        print(f"   - {len(df_corr)} paires de corr√©lations")
    
    # Export JSON
    if output_json:
        export_data = {
            'metadata': {
                'n_metrics': df_clean.shape[1],
                'n_timesteps': len(df_clean),
                'n_pairs': len(all_correlations),
                'metrics_analyzed': list(df_clean.columns)
            },
            'correlations': all_correlations,
            'summary': {
                'very_strong': sum(1 for c in all_correlations if abs(c['correlation']) > 0.9),
                'strong': sum(1 for c in all_correlations if 0.7 < abs(c['correlation']) <= 0.9),
                'moderate': sum(1 for c in all_correlations if 0.5 < abs(c['correlation']) <= 0.7),
                'weak': sum(1 for c in all_correlations if 0.3 < abs(c['correlation']) <= 0.5),
                'very_weak': sum(1 for c in all_correlations if abs(c['correlation']) <= 0.3)
            }
        }
        
        with open(output_json, 'w') as f:
            json.dump(export_data, f, indent=2)
        
        print(f"‚úÖ JSON sauvegard√©: {output_json}")
    
    # Statistiques
    print(f"\nR√©sum√©:")
    print(f"  - Corr√©lations tr√®s fortes (>0.9): {sum(1 for c in all_correlations if abs(c['correlation']) > 0.9)}")
    print(f"  - Corr√©lations fortes (0.7-0.9): {sum(1 for c in all_correlations if 0.7 < abs(c['correlation']) <= 0.9)}")
    print(f"  - Corr√©lations mod√©r√©es (0.5-0.7): {sum(1 for c in all_correlations if 0.5 < abs(c['correlation']) <= 0.7)}")
    
    return df_corr


def find_correlations_with_metric(df_correlations: pd.DataFrame, 
                                 metric_name: str,
                                 min_strength: float = 0.5,
                                 output_csv: str = None,
                                 output_json: str = None):
    """
    Trouve toutes les corr√©lations impliquant une m√©trique sp√©cifique.
    
    Args:
        df_correlations: DataFrame retourn√© par export_all_correlations
        metric_name: nom de la m√©trique √† chercher
        min_strength: corr√©lation minimale (en valeur absolue)
        output_csv: chemin pour exporter en CSV (None = pas d'export)
        output_json: chemin pour exporter en JSON (None = pas d'export)
        
    Returns:
        DataFrame: corr√©lations filtr√©es
    """
    if df_correlations is None:
        print("‚ö†Ô∏è Pas de donn√©es de corr√©lations")
        return None
    
    # Filtrer les lignes o√π metric_name appara√Æt
    mask = (
        (df_correlations['metric_1'] == metric_name) | 
        (df_correlations['metric_2'] == metric_name)
    ) & (df_correlations['abs_correlation'] >= min_strength)
    
    result = df_correlations[mask].copy()
    
    if len(result) == 0:
        print(f"‚ö†Ô∏è Aucune corr√©lation >= {min_strength} trouv√©e pour {metric_name}")
        return None
    
    # Normaliser : mettre metric_name toujours en premier
    normalized_result = []
    for _, row in result.iterrows():
        if row['metric_1'] == metric_name:
            other_metric = row['metric_2']
        else:
            other_metric = row['metric_1']
        
        normalized_result.append({
            'target_metric': metric_name,
            'correlated_with': other_metric,
            'correlation': row['correlation'],
            'abs_correlation': row['abs_correlation'],
            'correlation_type': row['correlation_type'],
            'strength': row['strength']
        })
    
    result_normalized = pd.DataFrame(normalized_result)
    result_normalized = result_normalized.sort_values('abs_correlation', ascending=False)
    
    print(f"üîç {len(result_normalized)} corr√©lations trouv√©es pour {metric_name}:")
    print(f"   (seuil minimum: {min_strength})")
    
    # Afficher le top 10
    for _, row in result_normalized.head(10).iterrows():
        print(f"   ‚Ä¢ {row['correlated_with']:25s}: {row['correlation']:+.3f} ({row['strength']})")
    
    if len(result_normalized) > 10:
        print(f"   ... et {len(result_normalized) - 10} autres")
    
    # Export CSV
    if output_csv:
        result_normalized.to_csv(output_csv, index=False)
        print(f"\n‚úÖ CSV sauvegard√©: {output_csv}")
    
    # Export JSON
    if output_json:
        export_data = {
            'metadata': {
                'target_metric': metric_name,
                'min_strength': min_strength,
                'n_correlations': len(result_normalized)
            },
            'correlations': normalized_result,
            'summary': {
                'very_strong': sum(1 for c in normalized_result if abs(c['correlation']) > 0.9),
                'strong': sum(1 for c in normalized_result if 0.7 < abs(c['correlation']) <= 0.9),
                'moderate': sum(1 for c in normalized_result if 0.5 < abs(c['correlation']) <= 0.7),
                'positive_count': sum(1 for c in normalized_result if c['correlation'] > 0),
                'negative_count': sum(1 for c in normalized_result if c['correlation'] < 0)
            }
        }
        
        with open(output_json, 'w') as f:
            json.dump(export_data, f, indent=2)
        
        print(f"‚úÖ JSON sauvegard√©: {output_json}")
    
    return result_normalized


print("‚úÖ Fonctions d'export de corr√©lations charg√©es!")
print("   - export_all_correlations() : Export CSV/JSON complet")
print("   - find_correlations_with_metric() : Recherche par m√©trique")


# ============== ANALYSE DE LA DIVERSIT√â DES STRATES ==============

def analyze_stratum_diversity(history: List[Dict], config: Dict):
    """
    Analyse la diversit√© des strates √† partir de l'historique de simulation.
    
    Args:
        history: historique complet de la simulation
        config: configuration (pour r√©cup√©rer N)
    
    Returns:
        dict avec statistiques par strate
    """
    if not history or len(history) < 10:
        print("‚ö†Ô∏è Pas assez d'historique pour analyser")
        return None
    
    # R√©cup√©rer le nombre de strates
    N = config['system']['N']
    
    results = {}
    
    print("\nüìä ANALYSE DE DIVERSIT√â PAR STRATE")
    print("="*80)
    
    # Pour chaque strate
    for n in range(N):
        # ‚úÖ Extraire les donn√©es depuis history
        An_values = []
        On_values = []
        fn_values = []
        error_values = []
        S_contrib_values = []
        
        for h in history:
            # R√©cup√©rer les arrays
            An = h.get('An', [])
            On = h.get('O', [])
            fn = h.get('fn', [])
            En = h.get('E', [])
            
            # V√©rifier que les indices existent
            if len(An) > n:
                An_values.append(An[n])
            if len(On) > n:
                On_values.append(On[n])
            if len(fn) > n:
                fn_values.append(fn[n])
            
            # Erreur = En - On
            if len(En) > n and len(On) > n:
                error_values.append(En[n] - On[n])
            
            # Contribution √† S (si disponible)
            S_contrib = h.get('S_contrib', [])
            if len(S_contrib) > n:
                S_contrib_values.append(S_contrib[n])
        
        # Si pas assez de donn√©es pour cette strate, skip
        if len(An_values) < 10:
            continue
        
        # Calculer les statistiques
        results[n] = {
            'An_mean': np.mean(An_values) if An_values else 0,
            'An_std': np.std(An_values) if An_values else 0,
            'An_max': np.max(An_values) if An_values else 0,
            'On_mean': np.mean(On_values) if On_values else 0,
            'On_std': np.std(On_values) if On_values else 0,
            'On_range': (np.max(On_values) - np.min(On_values)) if On_values else 0,
            'fn_mean': np.mean(fn_values) if fn_values else 0,
            'fn_final': fn_values[-1] if fn_values else 0,
            'error_mean': np.mean(error_values) if error_values else 0,
            'error_std': np.std(error_values) if error_values else 0,
            'S_contrib_mean': np.mean(S_contrib_values) if S_contrib_values else 0,
            'S_contrib_total': np.sum(S_contrib_values) if S_contrib_values else 0
        }
        
        # Affichage
        print(f"\nüìç Strate {n}:")
        print(f"   An:  {results[n]['An_mean']:.6f} ¬± {results[n]['An_std']:.6f}  (max: {results[n]['An_max']:.6f})")
        print(f"   On:  {results[n]['On_mean']:.6f} ¬± {results[n]['On_std']:.6f}  (range: {results[n]['On_range']:.6f})")
        print(f"   fn:  {results[n]['fn_mean']:.2f} ‚Üí {results[n]['fn_final']:.2f}")
        print(f"   Erreur: {results[n]['error_mean']:.6f} ¬± {results[n]['error_std']:.6f}")
        print(f"   Contrib S: {results[n]['S_contrib_mean']:.6f} (total: {results[n]['S_contrib_total']:.6f})")
    
    # ===== Analyse globale (identique) =====
    print("\n\nüìä ANALYSE GLOBALE")
    print("="*80)
    
    # Diversit√© des amplitudes
    An_means = [results[n]['An_mean'] for n in results.keys()]
    An_diversity = np.std(An_means) / (np.mean(An_means) + 1e-10)
    print(f"\n  Diversit√© des amplitudes (CV): {An_diversity:.3f}")
    
    # Annulation dans On
    On_means = [results[n]['On_mean'] for n in results.keys()]
    On_total = np.sum(On_means)
    On_abs_total = np.sum(np.abs(On_means))
    if On_abs_total > 1e-6:
        cancellation_ratio = 1 - abs(On_total) / On_abs_total
        print(f"  Annulation dans On: {cancellation_ratio*100:.1f}%")
        print(f"     (Œ£|On| = {On_abs_total:.6f}, Œ£ On = {On_total:.6f})")
    else:
        print(f"  On tr√®s faible partout (~0)")
    
    # Contributions √† S(t)
    S_contribs = [results[n]['S_contrib_total'] for n in results.keys()]
    S_total = np.sum(S_contribs)
    print(f"\n  Contribution totale √† S(t): {S_total:.6f}")
    
    # Strates dominantes
    if len(S_contribs) > 0:
        top_contrib_idx = np.argsort(np.abs(S_contribs))[-min(3, len(S_contribs)):][::-1]
        print(f"\n  Top 3 contributeurs √† S(t):")
        for idx in top_contrib_idx:
            print(f"     Strate {idx}: {S_contribs[idx]:.6f}")
    
    return results





# ============== UTILITAIRES ==============

def load_config_for_exploration() -> Dict:
    """
    Charge la configuration depuis config.json.
    """
    try:
        with open('config.json', 'r') as f:
            return deep_convert(json.load(f))
    except:
        # Configuration par d√©faut
        return deep_convert({
            'exploration': {
                'metrics': ['S(t)', 'C(t)', 'effort(t)'],
                'window_sizes': [1, 10, 100],
                'fractal_threshold': 0.8,
                'detect_fractal_patterns': True,
                'detect_anomalies': True,
                'detect_harmonics': True,
                'anomaly_threshold': 3.0,
                'min_duration': 3
            }
        })


def extract_run_id(file_path: str) -> str:
    """
    Extrait le run_id du nom de fichier.
    """
    basename = os.path.basename(file_path)
    # Essayer diff√©rents patterns
    if 'run_' in basename:
        parts = basename.split('run_')[1].split('.')[0]
        return f"run_{parts}"
    else:
        return basename.split('.')[0]


def count_events_by_type(events: List[Dict]) -> Dict[str, int]:
    """
    Compte les √©v√©nements par type.
    """
    counts = defaultdict(int)
    for event in events:
        counts[event['event_type']] += 1
    return dict(counts)


def classify_severity(value: float, threshold: float) -> str:
    """
    Classifie la s√©v√©rit√© d'un √©v√©nement.
    """
    if value < threshold * 1.5:
        return 'low'
    elif value < threshold * 3:
        return 'medium'
    else:
        return 'high'


def extract_seed_from_filename(file_path: str) -> Optional[int]:
    """
    üîß NOUVELLE FONCTION : Extrait la seed du nom de fichier.
    
    Formats support√©s :
    - run_20250622-232702_seed12345.csv
    - logs/run_*_seed12346.csv
    
    Returns:
        Seed extraite ou None
    """
    import re
    
    # Extraire seed du pattern _seed12345
    pattern = r'_seed(\d+)'
    match = re.search(pattern, os.path.basename(file_path))
    
    if match:
        return int(match.group(1))
    
    return None


def add_exploration_diversity(data: Dict[str, np.ndarray], seed: int) -> Dict[str, np.ndarray]:
    """
    üîß NOUVELLE FONCTION : Ajoute une l√©g√®re diversit√© aux donn√©es pour diff√©rencier les explorations.
    
    Ajoute un tr√®s petit bruit (< 0.1% de l'amplitude) pour √©viter que des runs avec patterns
    similaires donnent exactement les m√™mes r√©sultats d'exploration.
    
    Args:
        data: donn√©es originales
        seed: seed pour la randomisation
        
    Returns:
        donn√©es avec l√©g√®re diversification
    """
    diversified_data = data.copy()
    np.random.seed(seed)
    
    # Ajouter une petite diversification pour √©viter patterns identiques
    for key, values in data.items():
        if isinstance(values, np.ndarray) and values.dtype in [np.float64, np.float32]:
            if len(values) > 0:
                # Ajouter bruit tr√®s faible (0.01% de la std)
                noise_amplitude = np.std(values) * 0.0001
                noise = np.random.normal(0, noise_amplitude, len(values))
                diversified_data[key] = values + noise
    
    return diversified_data


# ============== TESTS ET VALIDATION ==============

if __name__ == "__main__":
    """
    Tests du module explore.py
    """
    print("=== Tests du module explore.py ===\n")
    
    # Cr√©er des donn√©es de test
    print("Test 1 - G√©n√©ration de donn√©es synth√©tiques:")
    t = np.linspace(0, 100, 1000)
    
    # Signal avec anomalies, bifurcations et fractales
    S_test = np.sin(2 * np.pi * t / 10)  # Signal de base
    S_test[200:220] += 5.0  # Anomalie
    S_test[500:] += 0.5 * np.sin(2 * np.pi * t[500:] / 3)  # Nouvelle harmonique
    
    # Ajouter du bruit fractal
    for scale in [1, 10, 100]:
        S_test += 0.1 / scale * np.sin(2 * np.pi * t * scale)
    
    C_test = np.cos(2 * np.pi * t / 15)
    C_test[400] += 3.0  # Saut de phase
    
    effort_test = 0.5 + 0.3 * np.sin(2 * np.pi * t / 20)
    effort_test[300:350] = 2.5  # Effort √©lev√©
    
    test_data = {
        'S(t)': S_test,
        'C(t)': C_test,
        'effort(t)': effort_test
    }
    
    print("  ‚úì Donn√©es synth√©tiques cr√©√©es")
    
    # Test d√©tection d'anomalies
    print("\nTest 2 - D√©tection d'anomalies:")
    anomalies = detect_anomalies(test_data, ['S(t)', 'effort(t)'], 3.0, 3)
    print(f"  ‚Üí {len(anomalies)} anomalies d√©tect√©es")
    
    # Test bifurcations
    print("\nTest 3 - D√©tection de bifurcations:")
    bifurcations = detect_spiral_bifurcations(test_data, 'C(t)', np.pi)
    print(f"  ‚Üí {len(bifurcations)} bifurcations d√©tect√©es")
    
    # Test harmoniques
    print("\nTest 4 - √âmergences harmoniques:")
    harmonics = detect_harmonic_emergence(test_data, 'S(t)', 5, 100, 10)
    print(f"  ‚Üí {len(harmonics)} √©mergences harmoniques")
    
    # Test fractales
    print("\nTest 5 - Motifs fractals:")
    fractals = detect_fractal_patterns(test_data, ['S(t)'], [1, 10, 100], 0.7)
    print(f"  ‚Üí {len(fractals)} motifs fractals")
    
    # Test rapport
    print("\nTest 6 - G√©n√©ration rapport:")
    all_test_events = anomalies + bifurcations + harmonics + fractals
    
    os.makedirs("test_output", exist_ok=True)
    generate_report(all_test_events, "test_output/test_report.md", "test_run", {})
    print("  ‚úì Rapport g√©n√©r√© : test_output/test_report.md")
    
    print("\n‚úÖ Module explore.py pr√™t √† r√©v√©ler l'invisible!")
