"""
utils.py - Fonctions utilitaires pour le syst√®me FPS
Version compl√®te conforme √† la feuille de route FPS V1.3
---------------------------------------------------------------
Ce module contient toutes les fonctions transversales qui
facilitent l'orchestration du syst√®me FPS :

- Gestion des logs et fusion de donn√©es
- Sauvegarde et restauration d'√©tats
- Ex√©cution parall√®le de runs
- Exports en formats multiples
- G√©n√©ration d'identifiants uniques
- Gestion de la structure des dossiers

Chaque fonction est con√ßue pour la robustesse, la tra√ßabilit√©
et la facilit√© d'utilisation dans l'√©cosyst√®me FPS.

(c) 2025 Gepetto & Andr√©a Gadal & Claude (Anthropic) üåÄ
"""

import os
import csv
import json
import pickle
import hashlib
import shutil
import glob
from datetime import datetime
from typing import Dict, List, Union, Optional, Any, Tuple
import numpy as np
import pandas as pd
from multiprocessing import Pool, cpu_count
import warnings
import traceback
from pathlib import Path

# Import optionnel pour HDF5
try:
    import h5py
    HDF5_AVAILABLE = True
except ImportError:
    HDF5_AVAILABLE = False
    warnings.warn("h5py non disponible - fonctionnalit√©s HDF5 d√©sactiv√©es")


# ============== GESTION DES LOGS ==============

def merge_logs(log_files: List[str], output_path: str, 
               format: str = 'csv') -> str:
    """
    Fusionne plusieurs fichiers de logs CSV en un seul.
    
    Args:
        log_files: liste des chemins vers les fichiers CSV
        output_path: chemin de sortie
        format: format de sortie ('csv' ou 'parquet')
    
    Returns:
        str: chemin du fichier fusionn√©
    """
    print(f"üîÑ Fusion de {len(log_files)} fichiers de logs...")
    
    # Charger tous les DataFrames
    dfs = []
    for log_file in log_files:
        try:
            df = pd.read_csv(log_file)
            # Ajouter une colonne avec le nom du fichier source
            df['source_file'] = os.path.basename(log_file)
            dfs.append(df)
            print(f"  ‚úì Charg√©: {os.path.basename(log_file)} ({len(df)} lignes)")
        except Exception as e:
            print(f"  ‚úó Erreur avec {log_file}: {e}")
    
    if not dfs:
        raise ValueError("Aucun fichier de log valide trouv√©")
    
    # Fusionner
    merged_df = pd.concat(dfs, ignore_index=True)
    
    # Trier par temps si la colonne existe
    if 't' in merged_df.columns:
        merged_df = merged_df.sort_values('t')
    
    # Sauvegarder
    os.makedirs(os.path.dirname(output_path) if os.path.dirname(output_path) else ".", 
                exist_ok=True)
    
    if format == 'csv':
        merged_df.to_csv(output_path, index=False)
    elif format == 'parquet' and 'pyarrow' in pd.io.parquet.get_engine('auto'):
        merged_df.to_parquet(output_path, index=False)
    else:
        # Fallback sur CSV
        output_path = output_path.replace('.parquet', '.csv')
        merged_df.to_csv(output_path, index=False)
    
    print(f"‚úÖ Fusion termin√©e: {output_path} ({len(merged_df)} lignes totales)")
    return output_path


def log_seed(seed: int, seed_file: str = "seeds.txt") -> None:
    """
    Enregistre une seed utilis√©e avec timestamp.
    
    Args:
        seed: valeur de la seed
        seed_file: fichier de log des seeds
    """
    os.makedirs(os.path.dirname(seed_file) if os.path.dirname(seed_file) else ".", 
                exist_ok=True)
    
    timestamp = datetime.now().strftime("%Y-%m-%d %H:%M:%S")
    with open(seed_file, 'a') as f:
        f.write(f"{timestamp} | SEED = {seed}\n")


def log_config_and_meta(config: Dict, run_id: str, 
                        output_dir: str = "logs") -> None:
    """
    Sauvegarde la configuration et les m√©tadonn√©es d'un run.
    
    Args:
        config: configuration compl√®te
        run_id: identifiant du run
        output_dir: dossier de sortie
    """
    os.makedirs(output_dir, exist_ok=True)
    
    # Sauvegarder la config
    config_path = os.path.join(output_dir, f"config_{run_id}.json")
    with open(config_path, 'w') as f:
        json.dump(deep_convert(config), f, indent=2)
    
    # Cr√©er un fichier de m√©tadonn√©es
    meta_path = os.path.join(output_dir, f"meta_{run_id}.json")
    metadata = {
        'run_id': run_id,
        'timestamp': datetime.now().isoformat(),
        'fps_version': '1.3',
        'config_path': config_path,
        'system': {
            'N': config.get('system', {}).get('N'),
            'T': config.get('system', {}).get('T'),
            'mode': config.get('system', {}).get('mode'),
            'seed': config.get('system', {}).get('seed')
        }
    }
    
    with open(meta_path, 'w') as f:
        json.dump(deep_convert(metadata), f, indent=2)
    
    print(f"üìù Configuration et m√©tadonn√©es sauvegard√©es pour {run_id}")


def log_end_of_run(run_id: str, summary: Optional[Dict] = None,
                   log_file: str = "runs_completed.txt") -> None:
    """
    Enregistre la fin d'un run avec r√©sum√© optionnel.
    
    Args:
        run_id: identifiant du run
        summary: r√©sum√© des r√©sultats
        log_file: fichier de log
    """
    timestamp = datetime.now().strftime("%Y-%m-%d %H:%M:%S")
    
    with open(log_file, 'a') as f:
        f.write(f"\n[{timestamp}] Run termin√©: {run_id}\n")
        
        if summary:
            f.write(f"  R√©sum√©:\n")
            for key, value in summary.items():
                if isinstance(value, (int, float)):
                    f.write(f"    - {key}: {value:.4f}\n")
                else:
                    f.write(f"    - {key}: {value}\n")


# ============== SAUVEGARDE ET RESTAURATION ==============

def save_simulation_state(state: Dict[str, Any], checkpoint_path: str) -> None:
    """
    Sauvegarde l'√©tat complet de la simulation.
    
    Args:
        state: √©tat du syst√®me (strates, historique, etc.)
        checkpoint_path: chemin du checkpoint
    """
    os.makedirs(os.path.dirname(checkpoint_path) if os.path.dirname(checkpoint_path) else ".", 
                exist_ok=True)
    
    # Sauvegarder avec pickle (qui g√®re les types numpy)
    with open(checkpoint_path, 'wb') as f:
        pickle.dump(state, f, protocol=pickle.HIGHEST_PROTOCOL)
    
    # NE PAS cr√©er de version JSON qui cause des erreurs
    # On peut cr√©er juste un fichier d'info minimal
    info_path = checkpoint_path.replace('.pkl', '_info.txt')
    with open(info_path, 'w') as f:
        f.write(f"Checkpoint cr√©√© : {datetime.now().isoformat()}\n")
        f.write(f"Chemin : {checkpoint_path}\n")
        if 'strates' in state:
            f.write(f"Nombre de strates : {len(state.get('strates', []))}\n")
        if 't' in state:
            f.write(f"Temps actuel : {state.get('t', 0)}\n")
    
    print(f"üíæ √âtat sauvegard√©: {checkpoint_path}")


def load_simulation_state(checkpoint_path: str) -> Dict[str, Any]:
    """
    Charge un √©tat de simulation sauvegard√©.
    
    Args:
        checkpoint_path: chemin du checkpoint
    
    Returns:
        Dict: √©tat restaur√©
    """
    if not os.path.exists(checkpoint_path):
        raise FileNotFoundError(f"Checkpoint non trouv√©: {checkpoint_path}")
    
    with open(checkpoint_path, 'rb') as f:
        state = pickle.load(f)
    
    print(f"‚úÖ √âtat restaur√© depuis: {checkpoint_path}")
    return state


# ============== REPLAY ET ANALYSE ==============

def replay_from_logs(csv_path: str, start_t: float = 0, 
                     end_t: Optional[float] = None) -> Dict[str, np.ndarray]:
    """
    Rejoue une simulation depuis les logs CSV.
    
    Args:
        csv_path: chemin vers le fichier CSV
        start_t: temps de d√©but
        end_t: temps de fin (None = jusqu'√† la fin)
    
    Returns:
        Dict: donn√©es recharg√©es
    """
    print(f"üîÑ Replay depuis: {csv_path}")
    
    # Charger le CSV
    df = pd.read_csv(csv_path)
    
    # Filtrer par temps si n√©cessaire
    if 't' in df.columns:
        if end_t is not None:
            df = df[(df['t'] >= start_t) & (df['t'] <= end_t)]
        else:
            df = df[df['t'] >= start_t]
    
    # Convertir en dictionnaire de arrays
    data = {}
    for col in df.columns:
        if col != 'effort_status':  # Exclure les colonnes non num√©riques
            try:
                data[col] = df[col].values.astype(float)
            except:
                # Garder comme string si non num√©rique
                data[col] = df[col].values
    
    print(f"  ‚úì Charg√©: {len(df)} pas de temps, {len(data)} m√©triques")
    return data


def compare_runs(run1_path: str, run2_path: str, 
                 metrics: List[str]) -> Dict[str, Dict[str, float]]:
    """
    Compare deux runs sur des m√©triques sp√©cifiques.
    
    Args:
        run1_path: chemin du premier run
        run2_path: chemin du second run
        metrics: liste des m√©triques √† comparer
    
    Returns:
        Dict: comparaison des m√©triques
    """
    # Charger les donn√©es
    data1 = replay_from_logs(run1_path)
    data2 = replay_from_logs(run2_path)
    
    comparison = {}
    
    for metric in metrics:
        if metric in data1 and metric in data2:
            values1 = data1[metric]
            values2 = data2[metric]
            
            comparison[metric] = {
                'run1_mean': np.mean(values1),
                'run2_mean': np.mean(values2),
                'run1_std': np.std(values1),
                'run2_std': np.std(values2),
                'difference_mean': np.mean(values1) - np.mean(values2),
                'correlation': np.corrcoef(values1[:min(len(values1), len(values2))], 
                                          values2[:min(len(values1), len(values2))])[0, 1]
            }
    
    return deep_convert(comparison)


# ============== EX√âCUTION PARALL√àLE ==============

def run_single_simulation(args: Tuple[str, Dict]) -> Dict[str, Any]:
    """
    Fonction worker pour ex√©cuter une simulation unique.
    
    Args:
        args: tuple (config_path, override_params)
    
    Returns:
        Dict: r√©sultats de la simulation
    """
    config_path, override_params = args
    
    try:
        # Importer les modules n√©cessaires
        import simulate
        import init
        
        # Charger la config
        config = init.load_config(config_path)
        
        # Appliquer les overrides
        for key, value in override_params.items():
            keys = key.split('.')
            target = config
            for k in keys[:-1]:
                target = target[k]
            target[keys[-1]] = value
        
        # Lancer la simulation
        result = simulate.run_simulation(config_path, config['system'].get('mode', 'FPS'))
        
        return deep_convert({
            'status': 'success',
            'run_id': result['run_id'],
            'metrics': result['metrics']
        })
        
    except Exception as e:
        return {
            'status': 'error',
            'error': str(e),
            'traceback': traceback.format_exc()
        }


def batch_runner(configs_list: List[Union[str, Tuple[str, Dict]]], 
                 parallel: bool = True, n_workers: Optional[int] = None) -> List[Dict]:
    """
    Execute un batch de simulations en parall√®le ou s√©quentiellement.
    
    Args:
        configs_list: liste de configs ou tuples (config_path, overrides)
        parallel: ex√©cution parall√®le ou non
        n_workers: nombre de workers (None = nb de CPU)
    
    Returns:
        List[Dict]: r√©sultats de toutes les simulations
    """
    print(f"\nüöÄ Lancement batch: {len(configs_list)} simulations")
    
    # Normaliser les inputs
    normalized_configs = []
    for config in configs_list:
        if isinstance(config, str):
            normalized_configs.append((config, {}))
        else:
            normalized_configs.append(config)
    
    results = []
    
    if parallel:
        # Ex√©cution parall√®le
        n_workers = n_workers or cpu_count()
        print(f"  Mode parall√®le avec {n_workers} workers")
        
        with Pool(n_workers) as pool:
            results = pool.map(run_single_simulation, normalized_configs)
    else:
        # Ex√©cution s√©quentielle
        print("  Mode s√©quentiel")
        for config in normalized_configs:
            result = run_single_simulation(config)
            results.append(result)
            
            # Afficher le statut
            if result['status'] == 'success':
                print(f"  ‚úì {result['run_id']}")
            else:
                print(f"  ‚úó Erreur: {result['error']}")
    
    # R√©sum√©
    success_count = sum(1 for r in results if r['status'] == 'success')
    print(f"\nüìä Batch termin√©: {success_count}/{len(results)} succ√®s")
    
    return deep_convert(results)


# ============== EXPORT DE DONN√âES ==============

def export_to_hdf5(data_dict: Dict[str, np.ndarray], hdf5_path: str) -> None:
    """
    Exporte des donn√©es volumineuses en format HDF5.
    
    Args:
        data_dict: dictionnaire de donn√©es √† exporter
        hdf5_path: chemin du fichier HDF5
    """
    if not HDF5_AVAILABLE:
        warnings.warn("HDF5 non disponible - export annul√©")
        return
    
    os.makedirs(os.path.dirname(hdf5_path) if os.path.dirname(hdf5_path) else ".", 
                exist_ok=True)
    
    with h5py.File(hdf5_path, 'w') as f:
        # M√©tadonn√©es
        f.attrs['created'] = datetime.now().isoformat()
        f.attrs['fps_version'] = '1.3'
        
        # Donn√©es
        for key, data in data_dict.items():
            if isinstance(data, np.ndarray):
                f.create_dataset(key, data=data, compression='gzip')
            elif isinstance(data, (list, tuple)):
                f.create_dataset(key, data=np.array(data), compression='gzip')
            else:
                # Convertir en array si possible
                try:
                    f.create_dataset(key, data=np.array([data]))
                except:
                    # Stocker comme attribut si non convertible
                    f.attrs[key] = str(data)
    
    # V√©rifier la taille
    file_size = os.path.getsize(hdf5_path) / (1024 * 1024)  # MB
    print(f"üíæ Export HDF5: {hdf5_path} ({file_size:.1f} MB)")


# ============== G√âN√âRATION D'IDENTIFIANTS ==============

def generate_run_id(prefix: str = "run") -> str:
    """
    G√©n√®re un identifiant unique pour un run.
    
    Args:
        prefix: pr√©fixe de l'identifiant
    
    Returns:
        str: identifiant unique
    """
    timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
    
    # Ajouter un hash court pour unicit√©
    random_bytes = os.urandom(4)
    hash_suffix = hashlib.md5(random_bytes).hexdigest()[:6]
    
    return f"{prefix}_{timestamp}_{hash_suffix}"


# ============== GESTION DES DOSSIERS ==============

def setup_directories(base_dir: str = "fps_output") -> Dict[str, str]:
    """
    Cr√©e la structure de dossiers pour les outputs FPS.
    
    Args:
        base_dir: dossier de base
    
    Returns:
        Dict: chemins cr√©√©s
    """
    timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
    run_dir = os.path.join(base_dir, f"run_{timestamp}")
    
    directories = {
        'base': run_dir,
        'logs': os.path.join(run_dir, 'logs'),
        'checkpoints': os.path.join(run_dir, 'checkpoints'),
        'figures': os.path.join(run_dir, 'figures'),
        'reports': os.path.join(run_dir, 'reports'),
        'configs': os.path.join(run_dir, 'configs')
    }
    
    for dir_path in directories.values():
        os.makedirs(dir_path, exist_ok=True)
    
    print(f"üìÅ Structure cr√©√©e: {run_dir}")
    return directories


def archive_run(run_dir: str, archive_name: Optional[str] = None) -> str:
    """
    Archive un dossier de run complet.
    
    Args:
        run_dir: dossier √† archiver
        archive_name: nom de l'archive (auto-g√©n√©r√© si None)
    
    Returns:
        str: chemin de l'archive
    """
    if archive_name is None:
        archive_name = f"{os.path.basename(run_dir)}_archive"
    
    # Cr√©er l'archive
    archive_path = shutil.make_archive(
        archive_name,
        'zip',
        os.path.dirname(run_dir),
        os.path.basename(run_dir)
    )
    
    print(f"üì¶ Archive cr√©√©e: {archive_path}")
    return archive_path


# ============== CHECKSUM ET INT√âGRIT√â ==============

def compute_checksum(file_path: str, algorithm: str = 'sha256') -> str:
    """
    Calcule le checksum d'un fichier pour v√©rifier l'int√©grit√©.
    
    Args:
        file_path: chemin du fichier
        algorithm: algorithme de hash ('md5', 'sha256', etc.)
    
    Returns:
        str: checksum hexad√©cimal
    """
    hash_func = getattr(hashlib, algorithm)()
    
    with open(file_path, 'rb') as f:
        for chunk in iter(lambda: f.read(4096), b""):
            hash_func.update(chunk)
    
    return hash_func.hexdigest()


def verify_data_integrity(data_dir: str, checksum_file: str = "checksums.txt") -> bool:
    """
    V√©rifie l'int√©grit√© des donn√©es d'un dossier.
    
    Args:
        data_dir: dossier contenant les donn√©es
        checksum_file: fichier de checksums
    
    Returns:
        bool: True si int√©grit√© v√©rifi√©e
    """
    checksum_path = os.path.join(data_dir, checksum_file)
    
    if not os.path.exists(checksum_path):
        print("‚ö†Ô∏è  Fichier de checksums non trouv√©")
        return False
    
    # Lire les checksums attendus
    expected_checksums = {}
    with open(checksum_path, 'r') as f:
        for line in f:
            if line.strip():
                parts = line.strip().split('  ')
                if len(parts) == 2:
                    expected_checksums[parts[1]] = parts[0]
    
    # V√©rifier chaque fichier
    all_valid = True
    for filename, expected in expected_checksums.items():
        file_path = os.path.join(data_dir, filename)
        if os.path.exists(file_path):
            actual = compute_checksum(file_path)
            if actual != expected:
                print(f"‚ùå Checksum invalide: {filename}")
                all_valid = False
            else:
                print(f"‚úì {filename}")
        else:
            print(f"‚ùå Fichier manquant: {filename}")
            all_valid = False
    
    return all_valid


# ============== GESTION DES ERREURS ==============

def handle_crash_recovery(state: Dict[str, Any], loggers: Dict,
                         exception: Exception) -> None:
    """
    G√®re la r√©cup√©ration apr√®s un crash.
    
    Args:
        state: √©tat du syst√®me au moment du crash
        loggers: informations de logging
        exception: exception lev√©e
    """
    crash_dir = "crash_recovery"
    os.makedirs(crash_dir, exist_ok=True)
    
    timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
    crash_id = f"crash_{timestamp}"
    
    # Fonction helper pour convertir les types numpy
    def convert_numpy_to_python(obj):
        """Convertit r√©cursivement les types numpy en types Python natifs."""
        if isinstance(obj, np.integer):
            return int(obj)
        elif isinstance(obj, np.floating):
            return float(obj)
        elif isinstance(obj, np.ndarray):
            return obj.tolist()
        elif isinstance(obj, dict):
            return {k: convert_numpy_to_python(v) for k, v in obj.items()}
        elif isinstance(obj, list):
            return [convert_numpy_to_python(item) for item in obj]
        else:
            return obj
    
    # Sauvegarder l'√©tat avec pickle (qui g√®re les types numpy)
    state_path = os.path.join(crash_dir, f"{crash_id}_state.pkl")
    try:
        save_simulation_state(state, state_path)
    except Exception as e:
        print(f"‚ö†Ô∏è Erreur sauvegarde √©tat pickle: {e}")
    
    # Convertir l'√©tat pour JSON
    state_for_json = deep_convert(state)
    
    # Sauvegarder les d√©tails du crash
    crash_info = {
        'timestamp': timestamp,
        'run_id': loggers.get('run_id', 'unknown'),
        'exception_type': type(exception).__name__,
        'exception_message': str(exception),
        'traceback': traceback.format_exc(),
        't_current': float(state_for_json.get('t', 0)) if 't' in state_for_json else 'unknown',
        'n_strates': len(state_for_json.get('strates', [])),
        'mode': state_for_json.get('mode', 'unknown')
    }
    
    # Ajouter les m√©triques si disponibles
    if 'all_metrics' in state_for_json:
        crash_info['last_metrics'] = state_for_json['all_metrics']
    
    info_path = os.path.join(crash_dir, f"{crash_id}_info.json")
    try:
        with open(info_path, 'w') as f:
            json.dump(crash_info, f, indent=2)
    except Exception as e:
        print(f"‚ö†Ô∏è Erreur sauvegarde JSON: {e}")
        # Essayer sans les m√©triques
        crash_info.pop('last_metrics', None)
        with open(info_path, 'w') as f:
            json.dump(crash_info, f, indent=2)
    
    print(f"\nüö® Crash recovery:")
    print(f"  √âtat sauvegard√©: {state_path}")
    print(f"  Infos crash: {info_path}")
    print(f"  Pour reprendre: load_simulation_state('{state_path}')")


# ============== UTILITAIRES DIVERS ==============

def format_duration(seconds: float) -> str:
    """
    Formate une dur√©e en secondes en format lisible.
    
    Args:
        seconds: dur√©e en secondes
    
    Returns:
        str: dur√©e format√©e (ex: "2h 15m 30s")
    """
    hours = int(seconds // 3600)
    minutes = int((seconds % 3600) // 60)
    secs = seconds % 60
    
    parts = []
    if hours > 0:
        parts.append(f"{hours}h")
    if minutes > 0:
        parts.append(f"{minutes}m")
    if secs > 0 or len(parts) == 0:
        parts.append(f"{secs:.1f}s")
    
    return " ".join(parts)


def get_system_info() -> Dict[str, Any]:
    """
    R√©cup√®re des informations sur le syst√®me.
    
    Returns:
        Dict: informations syst√®me
    """
    import platform
    import psutil
    
    info = {
        'platform': platform.platform(),
        'python_version': platform.python_version(),
        'cpu_count': cpu_count(),
        'memory_total_gb': psutil.virtual_memory().total / (1024**3),
        'memory_available_gb': psutil.virtual_memory().available / (1024**3),
        'disk_usage_percent': psutil.disk_usage('/').percent
    }
    
    return info

def deep_convert(obj):
    """
    Convertit r√©cursivement tous les np.ndarray en list et tous les types numpy en types Python natifs.
    √Ä utiliser avant tout export JSON, logging de batchs ou rapport final.
    """
    import numpy as np
    if isinstance(obj, np.ndarray):
        return obj.tolist()
    elif isinstance(obj, dict):
        return {k: deep_convert(v) for k, v in obj.items()}
    elif isinstance(obj, list):
        return [deep_convert(x) for x in obj]
    elif isinstance(obj, (np.integer, np.floating)):
        return obj.item()
    else:
        return obj

def deep_convert_for_json(obj):
    """
    Convertit r√©cursivement un objet Python pour le rendre s√©rialisable en JSON.
    G√®re notamment les cl√©s tuples en les convertissant en strings.
    
    Args:
        obj: objet √† convertir
        
    Returns:
        objet converti compatible JSON
    """
    if isinstance(obj, dict):
        converted = {}
        for key, value in obj.items():
            # Convertir les cl√©s tuples en strings
            if isinstance(key, tuple):
                key_str = f"({','.join(str(k) for k in key)})"
                converted[key_str] = deep_convert_for_json(value)
            else:
                converted[str(key)] = deep_convert_for_json(value)
        return converted
    elif isinstance(obj, (list, tuple)):
        return [deep_convert_for_json(item) for item in obj]
    elif isinstance(obj, np.integer):
        return int(obj)
    elif isinstance(obj, np.floating):
        return float(obj)
    elif isinstance(obj, np.ndarray):
        return obj.tolist()
    elif hasattr(obj, '__dict__'):
        # Objets custom
        return deep_convert_for_json(obj.__dict__)
    else:
        return obj

# ============== TOPOLOGIES / COUPLAGE ==============


def generate_spiral_weights(N: int,
                            c: float = 0.25, c_edge: float = 0.25,
                            closed: bool = False,
                            mirror: bool = False) -> List[List[float]]:
    """Generate an antisymmetric weight matrix producing a spiral-like coupling.

    Each strate i influences the next strate i+1 with +c, while the next strate
    feeds back ‚àíc on i (antisymmetry ‚Üí Œ£ w[i] = 0 for every row).  If *closed*
    is True the last strate N-1 is connected back to 0 (ring); otherwise the
    extremities remain open, giving a genuine spiral.

    Parameters
    ----------
    N : int
        Number of strates.
    c : float, optional
        Coupling coefficient (>0).  Typical range 0.05 ‚Äì 0.30.
    closed : bool, optional
        Whether to close the spiral into a ring (True) or keep it open (False).
    mirror : bool, optional
        Whether to conserve sum by adjusting edge weights.

    Returns
    -------
    List[List[float]]
        Weight matrix W where W[i][j] is the influence of j on i.
    """
    import numpy as _np  # local import avoids polluting public namespace

    if N <= 1:
        return [[0.0]]  # trivial case
    
    if c_edge is None:
        c_edge = c

    W = _np.zeros((N, N))


    # Forward couplings
    for i in range(N - 1):
        W[i, i+1] = +c
        W[i, i-1] = -c

    # Optionally close the ring
    if closed and N > 2:
        W[N - 1, 0] = +c
        W[0, N - 1] = -c
    elif mirror and N > 2:
        # bords avec "retour miroir" antisym√©trique
        # ligne 0 : +c depuis 1, -c_edge depuis N-1
        W[0, 1]     = +c
        W[0, N-1]   = -c_edge

        # ligne N-1 : +c_edge depuis 0, -c depuis N-2
        W[N-1, 0]   = +c_edge
        W[N-1, N-2] = -c

        # antisym√©trie exacte (s√©curit√©)
        W = 0.5*(W - W.T)

    # Convert to plain Python lists (to be JSON-serialisable)
    return W.tolist()

# ============== TESTS ET VALIDATION ==============

if __name__ == "__main__":
    """
    Tests du module utils.py
    """
    print("=== Tests du module utils.py ===\n")
    
    # Test 1: G√©n√©ration d'ID
    print("Test 1 - G√©n√©ration d'identifiants:")
    for i in range(3):
        run_id = generate_run_id()
        print(f"  ID {i+1}: {run_id}")
    
    # Test 2: Gestion des dossiers
    print("\nTest 2 - Structure de dossiers:")
    dirs = setup_directories("test_fps_output")
    for key, path in dirs.items():
        print(f"  {key}: {path}")
    
    # Test 3: Sauvegarde de seed
    print("\nTest 3 - Log de seed:")
    test_seed = 42
    log_seed(test_seed, os.path.join(dirs['logs'], "seeds.txt"))
    print(f"  Seed {test_seed} logu√©e")
    
    # Test 4: Sauvegarde d'√©tat
    print("\nTest 4 - Sauvegarde/restauration d'√©tat:")
    test_state = {
        't': 50.0,
        'strates': [{'id': 0, 'An': 1.0}, {'id': 1, 'An': 0.8}],
        'history': [{'t': 0, 'S': 0}, {'t': 1, 'S': 0.5}]
    }
    
    checkpoint_path = os.path.join(dirs['checkpoints'], "test_checkpoint.pkl")
    save_simulation_state(test_state, checkpoint_path)
    
    restored_state = load_simulation_state(checkpoint_path)
    print(f"  √âtat restaur√©: t={restored_state['t']}, n_strates={len(restored_state['strates'])}")
    
    # Test 5: Configuration et m√©tadonn√©es
    print("\nTest 5 - Log de configuration:")
    test_config = {
        'system': {'N': 3, 'T': 100, 'mode': 'FPS', 'seed': 42},
        'strates': [{'A0': 1.0, 'f0': 1.0}] * 3
    }
    log_config_and_meta(test_config, "test_run", dirs['configs'])
    
    # Test 6: Checksum
    print("\nTest 6 - Checksum:")
    test_file = checkpoint_path
    checksum = compute_checksum(test_file)
    print(f"  SHA256: {checksum[:32]}...")
    
    # Test 7: Formatage de dur√©e
    print("\nTest 7 - Formatage de dur√©e:")
    durations = [45.3, 125.7, 3665.2, 7200.0]
    for d in durations:
        print(f"  {d}s ‚Üí {format_duration(d)}")
    
    # Test 8: Informations syst√®me
    print("\nTest 8 - Informations syst√®me:")
    try:
        sys_info = get_system_info()
        print(f"  Python: {sys_info['python_version']}")
        print(f"  CPUs: {sys_info['cpu_count']}")
        print(f"  RAM: {sys_info['memory_available_gb']:.1f}/{sys_info['memory_total_gb']:.1f} GB")
    except:
        print("  (psutil non disponible)")
    
    # Test 9: Archive
    print("\nTest 9 - Archivage:")
    archive_path = archive_run(dirs['base'])
    print(f"  Archive cr√©√©e: {archive_path}")
    
    # Nettoyage
    shutil.rmtree("test_fps_output", ignore_errors=True)
    if os.path.exists(archive_path):
        os.remove(archive_path)
    
    print("\n‚úÖ Module utils.py pr√™t √† orchestrer la symphonie FPS")


# ============== FONCTIONS ADAPTATIVES ==============

def save_coupled_discoveries(gamma_journal: Dict, regulation_state: Dict, 
                           output_path: str) -> None:
    """
    Sauvegarde les d√©couvertes coupl√©es (Œ≥, G) dans un fichier JSON.
    Si le fichier est trop gros (>15MB), le divise en plusieurs parties.
    
    Args:
        gamma_journal: journal des d√©couvertes gamma
        regulation_state: √©tat de la r√©gulation G
        output_path: chemin de sortie (sera adapt√© si division n√©cessaire)
    """
    import os
    import json
    from pathlib import Path
    from datetime import datetime
    
    # Pr√©parer les donn√©es
    discoveries = {
        'timestamp': datetime.now().isoformat(),
        'gamma_discoveries': gamma_journal,
        'G_discoveries': regulation_state
    }
    
    # Convertir pour JSON (tuples -> strings, etc.)
    discoveries_serializable = deep_convert_for_json(discoveries)
    
    # Convertir en JSON pour v√©rifier la taille
    json_str = json.dumps(discoveries_serializable, indent=2, ensure_ascii=False)
    size_mb = len(json_str.encode('utf-8')) / (1024 * 1024)
    
    # Si petit fichier, sauvegarder normalement
    if size_mb < 15:  # Limite √† 15MB pour garder une marge
        with open(output_path, 'w', encoding='utf-8') as f:
            f.write(json_str)
        return
    
    # Si gros fichier, cr√©er un dossier et diviser
    base_path = Path(output_path)
    folder_name = base_path.stem + "_parts"
    folder_path = base_path.parent / folder_name
    folder_path.mkdir(exist_ok=True)
    
    # Diviser les d√©couvertes en chunks
    # Strat√©gie : diviser par √©tats coupl√©s
    gamma_states = discoveries_serializable['gamma_discoveries'].get('coupled_states', {})
    
    if gamma_states:
        # Calculer combien d'√©tats par chunk pour rester sous 15MB
        total_states = len(gamma_states)
        estimated_states_per_chunk = max(1, int(total_states * 15 / size_mb))
        
        # Diviser les √©tats
        states_items = list(gamma_states.items())
        chunk_num = 0
        
        for i in range(0, total_states, estimated_states_per_chunk):
            chunk_states = dict(states_items[i:i + estimated_states_per_chunk])
            
            # Cr√©er un chunk avec m√©tadonn√©es
            chunk_data = {
                'timestamp': discoveries_serializable['timestamp'],
                'chunk_info': {
                    'part': chunk_num + 1,
                    'total_parts': (total_states + estimated_states_per_chunk - 1) // estimated_states_per_chunk,
                    'states_in_chunk': len(chunk_states),
                    'total_states': total_states
                },
                'gamma_discoveries': {
                    **{k: v for k, v in discoveries_serializable['gamma_discoveries'].items() if k != 'coupled_states'},
                    'coupled_states': chunk_states
                }
            }
            
            # Ajouter les d√©couvertes G seulement dans le premier chunk
            if chunk_num == 0:
                chunk_data['G_discoveries'] = discoveries_serializable['G_discoveries']
            
            # Sauvegarder le chunk
            chunk_path = folder_path / f"part_{chunk_num:03d}.json"
            with open(chunk_path, 'w', encoding='utf-8') as f:
                json.dump(chunk_data, f, indent=2, ensure_ascii=False)
            
            chunk_num += 1
        
        # Cr√©er un fichier index
        index_data = {
            'timestamp': discoveries_serializable['timestamp'],
            'total_parts': chunk_num,
            'total_states': total_states,
            'folder': str(folder_path),
            'original_size_mb': round(size_mb, 2),
            'parts': [f"part_{i:03d}.json" for i in range(chunk_num)]
        }
        
        index_path = folder_path / "index.json"
        with open(index_path, 'w', encoding='utf-8') as f:
            json.dump(index_data, f, indent=2, ensure_ascii=False)
        
        print(f"  üìÇ D√©couvertes divis√©es en {chunk_num} parties dans : {folder_path}")
        print(f"     Taille originale : {size_mb:.1f}MB ‚Üí ~{15:.1f}MB par partie")
    
    else:
        # Si pas de states √† diviser, sauvegarder tel quel avec warning
        with open(output_path, 'w', encoding='utf-8') as f:
            f.write(json_str)
        print(f"  ‚ö†Ô∏è  Fichier volumineux ({size_mb:.1f}MB) sauvegard√© sans division : {output_path}")


def select_representative_strata(N, config=None, n_strata_to_show=None):
    """
    S√©lectionne des strates repr√©sentatives pour visualisation.
    
    Args:
        N: Nombre total de strates
        config: Configuration (optionnel)
        n_strata_to_show: Nombre exact de strates √† montrer (override config)
    
    Returns:
        list: Indices des strates s√©lectionn√©es, r√©partis uniform√©ment
    """
    # D√©terminer combien de strates √† montrer
    if n_strata_to_show is not None:
        # Override explicite
        n_show = n_strata_to_show
    elif config and 'visualization' in config:
        # Depuis config (pourcentage ou nombre absolu)
        viz_config = config['visualization']
        
        if 'strata_sample_percent' in viz_config:
            # Pourcentage (ex: 0.2 = 20%)
            percent = viz_config['strata_sample_percent']
            n_show = max(1, int(N * percent))
        elif 'strata_sample_count' in viz_config:
            # Nombre absolu
            n_show = viz_config['strata_sample_count']
        else:
            # D√©faut : 10% avec min 5, max 10
            n_show = max(5, min(10, N // 10))
    else:
        # D√©faut : adaptatif selon N
        if N <= 10:
            n_show = N  # Tout montrer
        elif N <= 50:
            n_show = 5  # 10%
        else:
            n_show = max(5, min(10, N // 10))
    
    # Limiter entre 1 et N
    n_show = max(1, min(n_show, N))
    
    # S√©lectionner les indices uniform√©ment r√©partis
    if n_show == 1:
        indices = [0]
    elif n_show == 2:
        indices = [0, N-1]
    elif n_show >= N:
        indices = list(range(N))
    else:
        # R√©partition uniforme : toujours inclure d√©but et fin
        indices = [0]  # Toujours la premi√®re
        
        # Strates interm√©diaires espac√©es uniform√©ment
        step = (N - 1) / (n_show - 1)
        for i in range(1, n_show - 1):
            idx = int(round(i * step))
            indices.append(idx)
        
        indices.append(N - 1)  # Toujours la derni√®re
    
    return indices
